{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a89642",
   "metadata": {},
   "source": [
    "load csv from downloads\n",
    "normalize and rename\n",
    "split train/test\n",
    "try logr and gbc baseline\n",
    "evaluate on accuracy (imbalanced?) -goal is 65%+\n",
    "\n",
    "try pytorch 2 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96d083ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a92185",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c030d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pi_raw = pd.read_csv(r\"C:\\Users\\Devan\\Downloads\\datasets\\pima-indians-diabetes.csv\")\n",
    "#print(df_pi_raw.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21550a2",
   "metadata": {},
   "source": [
    "Number of times pregnant. <br>\n",
    "Plasma glucose concentration a 2 hours in an oral glucose tolerance test. \n",
    "Diastolic blood pressure (mm Hg).\n",
    "Triceps skinfold thickness (mm).\n",
    "2-Hour serum insulin (mu U/ml).\n",
    "Body mass index (weight in kg/(height in m)^2).\n",
    "Diabetes pedigree function.\n",
    "Age (years).\n",
    "Class variable (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "348366be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['pregnant_count','plasma_gluc','bp','skinfold','insulin','bmi','pedigree','age','y_class']\n",
    "df_pi_raw.set_axis(cols, axis=1, inplace=True)\n",
    "#print(df_pi_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb043eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df_pi_X_std = pd.DataFrame(index=df_pi_raw.index)\n",
    "for c in df_pi_raw.columns:\n",
    "    if c != 'y_class':\n",
    "        new_col = c+'_std'\n",
    "        df_pi_X_std[new_col] = StandardScaler().fit_transform(df_pi_raw.loc[:,[c]])\n",
    "df_pi_Y = df_pi_raw.loc[:,['y_class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "045e5ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_pi_X_std, df_pi_Y\n",
    "                                                    ,train_size=.8\n",
    "                                                    ,test_size=.2\n",
    "                                                    ,stratify= df_pi_Y\n",
    "                                                    ,random_state=123\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b04f9524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 613 entries, 560 to 354\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   pregnant_count_std  613 non-null    float64\n",
      " 1   plasma_gluc_std     613 non-null    float64\n",
      " 2   bp_std              613 non-null    float64\n",
      " 3   skinfold_std        613 non-null    float64\n",
      " 4   insulin_std         613 non-null    float64\n",
      " 5   bmi_std             613 non-null    float64\n",
      " 6   pedigree_std        613 non-null    float64\n",
      " 7   age_std             613 non-null    float64\n",
      "dtypes: float64(8)\n",
      "memory usage: 59.3 KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b09c1",
   "metadata": {},
   "source": [
    "## LogR and GBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "16e52b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logr = LogisticRegression()\n",
    "logr.fit(X_train,np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "99843280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(max_depth=5, n_estimators=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(max_depth=5, n_estimators=50)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(max_depth=5, n_estimators=50)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators=50,max_depth=5)\n",
    "gbc.fit(X_train,np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d457f786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logr Acc score:  0.81\n",
      "GBC Acc score:   0.74\n",
      "Logr F1 score:   0.69\n",
      "GBC F1 score:    0.60\n",
      "Logr F1 w score: 0.80\n",
      "GBC F1 w score:  0.73\n"
     ]
    }
   ],
   "source": [
    "#Evaluation on Test\n",
    "from sklearn.metrics import f1_score\n",
    "print(f'Logr Acc score:  {logr.score(X_test,y_test):0.2f}')\n",
    "print(f'GBC Acc score:   {gbc.score(X_test,y_test):0.2f}')\n",
    "print(f'Logr F1 score:   {f1_score(y_test, logr.predict(X_test)):0.2f}')\n",
    "print(f'GBC F1 score:    {f1_score(y_test, gbc.predict(X_test)):0.2f}')\n",
    "logrf1_w = f1_score(y_test, logr.predict(X_test), average='weighted')\n",
    "gbcf1_w = f1_score(y_test, gbc.predict(X_test), average='weighted')\n",
    "print(f'Logr F1 w score: {logrf1_w:0.2f}')\n",
    "print(f'GBC F1 w score:  {gbcf1_w:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc7412",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe92f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0f9932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGR test\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(8, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.l1(x))\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27d2f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Net v1\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(8, 4)\n",
    "        #self.h1 = nn.Linear(6, 4)\n",
    "        self.o1 = nn.Linear(4, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        #x = F.relu(self.h1(x))  #relu too high, sig too low\n",
    "        #x = self.o1(x)  #ok by self\n",
    "        #x = F.softmax(self.o1(x))\n",
    "        x = F.sigmoid(self.o1(x))\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1283d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "532adfd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7156)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recreate data as a tensor tbd\n",
    "#torch.tensor(np.ravel(X_train.loc[111]))\n",
    "#torch.tensor(np.ravel(y_train.loc[111]+0.))\n",
    "#criterion(torch.tensor(.444), torch.tensor(1.0))\n",
    "#torch.tensor(np.ravel([y_train.loc[111]+0.,1-y_train.loc[111]+0.]))\n",
    "\n",
    "p = torch.tensor([.4778])\n",
    "t = torch.tensor([1.0])\n",
    "p = torch.tensor([.4778,1.0-.4778])\n",
    "t = torch.tensor([1.0, 1.0-1.0])\n",
    "criterion(p, t)\n",
    "#F.binary_cross_entropy(torch.tensor([.477,1-.4777]), torch.tensor([1.0,0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb0ccc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.669\n",
      "outputs=tensor([0.3990, 0.5901], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[1,   400] loss: 0.670\n",
      "outputs=tensor([0.3611, 0.5850], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[1,   600] loss: 0.660\n",
      "outputs=tensor([0.6164, 0.7218], grad_fn=<SigmoidBackward0>) // labels=tensor([1., 0.], dtype=torch.float64)\n",
      "[2,   200] loss: 0.655\n",
      "outputs=tensor([0.3601, 0.6459], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[2,   400] loss: 0.659\n",
      "outputs=tensor([0.3244, 0.6601], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[2,   600] loss: 0.648\n",
      "outputs=tensor([0.6451, 0.7234], grad_fn=<SigmoidBackward0>) // labels=tensor([1., 0.], dtype=torch.float64)\n",
      "[3,   200] loss: 0.641\n",
      "outputs=tensor([0.3250, 0.7010], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[3,   400] loss: 0.648\n",
      "outputs=tensor([0.2848, 0.7447], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[3,   600] loss: 0.635\n",
      "outputs=tensor([0.6883, 0.7126], grad_fn=<SigmoidBackward0>) // labels=tensor([1., 0.], dtype=torch.float64)\n",
      "[4,   200] loss: 0.626\n",
      "outputs=tensor([0.2864, 0.7576], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[4,   400] loss: 0.634\n",
      "outputs=tensor([0.2308, 0.8324], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[4,   600] loss: 0.621\n",
      "outputs=tensor([0.7358, 0.6842], grad_fn=<SigmoidBackward0>) // labels=tensor([1., 0.], dtype=torch.float64)\n",
      "[5,   200] loss: 0.609\n",
      "outputs=tensor([0.2405, 0.8131], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[5,   400] loss: 0.619\n",
      "outputs=tensor([0.1629, 0.9062], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[5,   600] loss: 0.607\n",
      "outputs=tensor([0.7796, 0.6355], grad_fn=<SigmoidBackward0>) // labels=tensor([1., 0.], dtype=torch.float64)\n",
      "[6,   200] loss: 0.591\n",
      "outputs=tensor([0.1912, 0.8602], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[6,   400] loss: 0.603\n",
      "outputs=tensor([0.0996, 0.9514], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[6,   600] loss: 0.593\n",
      "outputs=tensor([0.8110, 0.5672], grad_fn=<SigmoidBackward0>) // labels=tensor([1., 0.], dtype=torch.float64)\n",
      "[7,   200] loss: 0.573\n",
      "outputs=tensor([0.1464, 0.8953], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[7,   400] loss: 0.587\n",
      "outputs=tensor([0.0558, 0.9748], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[7,   600] loss: 0.581\n",
      "outputs=tensor([0.8275, 0.4896], grad_fn=<SigmoidBackward0>) // labels=tensor([1., 0.], dtype=torch.float64)\n",
      "[8,   200] loss: 0.558\n",
      "outputs=tensor([0.1103, 0.9210], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[8,   400] loss: 0.572\n",
      "outputs=tensor([0.0303, 0.9867], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[8,   600] loss: 0.571\n",
      "outputs=tensor([0.8316, 0.4230], grad_fn=<SigmoidBackward0>) // labels=tensor([1., 0.], dtype=torch.float64)\n",
      "[9,   200] loss: 0.545\n",
      "outputs=tensor([0.0840, 0.9389], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[9,   400] loss: 0.561\n",
      "outputs=tensor([0.0174, 0.9922], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[9,   600] loss: 0.564\n",
      "outputs=tensor([0.8293, 0.3752], grad_fn=<SigmoidBackward0>) // labels=tensor([1., 0.], dtype=torch.float64)\n",
      "[10,   200] loss: 0.536\n",
      "outputs=tensor([0.0655, 0.9515], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[10,   400] loss: 0.552\n",
      "outputs=tensor([0.0109, 0.9949], grad_fn=<SigmoidBackward0>) // labels=tensor([0., 1.], dtype=torch.float64)\n",
      "[10,   600] loss: 0.558\n",
      "outputs=tensor([0.8229, 0.3441], grad_fn=<SigmoidBackward0>) // labels=tensor([1., 0.], dtype=torch.float64)\n",
      "tensor([0.0698, 0.9439], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    #for i, data in enumerate(trainloader, 0):\n",
    "    for r in X_train.index:\n",
    "        ## get the inputs; data is a list of [inputs, labels]\n",
    "        #inputs, labels = data\n",
    "        inputs = torch.tensor(np.ravel(X_train.loc[r]))\n",
    "        labels = torch.tensor(np.ravel([y_train.loc[r]+0.,1.0-y_train.loc[r]+0.]))  #converted to onehot\n",
    "        #labels = torch.tensor(np.ravel(y_train.loc[r]+0.))\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        #loss = F.binary_cross_entropy(outputs.float(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "            running_loss = 0.0\n",
    "            #print(f'{outputs=} // {labels=}')\n",
    "        i = i+1\n",
    "\n",
    "print(net(torch.tensor(np.ravel(X_test.iloc[1])).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b299a342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0698, 0.9439], grad_fn=<SigmoidBackward0>)\n",
      "Actual 1\tPredicted: 1\n",
      "Actual 0\tPredicted: 1\n",
      "Actual 1\tPredicted: 0\n",
      "Actual 0\tPredicted: 1\n",
      "Actual 0\tPredicted: 0\n",
      "Actual 0\tPredicted: 1\n",
      "Actual 1\tPredicted: 1\n",
      "Actual 0\tPredicted: 1\n",
      "Actual 0\tPredicted: 1\n",
      "Actual 1\tPredicted: 1\n",
      "54\n",
      "119\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "pred_out = []\n",
    "print(net(torch.tensor(np.ravel(X_test.iloc[1])).float()))\n",
    "for r in X_test.index:\n",
    "    inputs = torch.tensor(np.ravel(X_test.loc[r]))\n",
    "    outputs = net(inputs.float())\n",
    "    #print(outputs)\n",
    "    if outputs[1] > outputs[0]:\n",
    "        pred_out.append(1)\n",
    "    else:\n",
    "        pred_out.append(0)\n",
    "\n",
    "for i in range(10):\n",
    "   print(\"Actual {}\\tPredicted: {}\".format(y_test.iloc[i].item(), pred_out[i]))\n",
    "\n",
    "print(y_test.sum().item()) #54 of 154\n",
    "print(pd.DataFrame(pred_out).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f47b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79a82940",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de77030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature interpretation\n",
    "#package install - tbd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
